{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name : Syed Khalid Ahmed\n",
    "#### Marticulation number : 276970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program gives suggestion for a word if it is written worngly. It asks the user how many number of suggestions he/she wants and then gives the output accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import udhr\n",
    "import math\n",
    "import operator\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import brown\n",
    "\n",
    "WordNgram = dict()\n",
    "ReverseLookup = dict()\n",
    "num_of_suggestions = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizes the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This function tokenizes a sentence and returns it\n",
    "def Tokenizer(sentence):\n",
    "\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Calculator function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the N-gram for given words in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculates the N-grams for given words in a list\n",
    "def NgramCalculator(tokens):\n",
    "\n",
    "    WordNgram = dict()  # Dictionary to store N-grams of a particular word\n",
    "    weight = 3          # generate 3-grams -- can be changed to any arbitrary number\n",
    "\n",
    "    for num_of_tokens in range(len(tokens)):    # Loop through all words in the list\n",
    "\n",
    "        keyValue = dict()\n",
    "\n",
    "        ## Find the N-grams for the word in the list\n",
    "        for token_length in range(len(tokens[num_of_tokens])):  \n",
    "            temp = tokens[num_of_tokens][token_length:token_length+weight]\n",
    "\n",
    "            ## If the n-gram already occured previously in the given word\n",
    "            if temp in keyValue:\n",
    "                keyValue[temp] += 1     # Increase count by 1\n",
    "            else:\n",
    "                keyValue[temp] = 1      # If appearing for first time then assign a value 1\n",
    "    \n",
    "        WordNgram[tokens[num_of_tokens]] = keyValue     # Assign the N-gram dictionary to that particular word\n",
    "\n",
    "    return WordNgram    # Return the Main Dictionary containing words with their N-grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dictionary which stores the N-gram as a key and the words in which that N-gram occurs as values. For example: If 'th' appears in 4 words then the key will be 'th' and value will contain those 4 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creates a dictionary which stores the N-gram as a key and the words in which\n",
    "## that N-gram occurs as values.\n",
    "## If 'th' appears in 4 words then the key will be 'th' and value will contain those 4 words\n",
    "def aggregator(dataset):\n",
    "\n",
    "    global ReverseLookup\n",
    "    \n",
    "    for parent_key, parent_value in dataset.items():\n",
    "\n",
    "        for child_key, child_value in parent_value.items():\n",
    "            \n",
    "            if child_key in ReverseLookup:\n",
    "    \n",
    "                if parent_key in ReverseLookup[child_key]:\n",
    "                    \n",
    "                    ReverseLookup[child_key][parent_key] += child_value\n",
    "                else:\n",
    "                    \n",
    "                    ReverseLookup[child_key].update({parent_key : child_value})                 \n",
    "            else:\n",
    "                ReverseLookup[child_key] = {parent_key : child_value}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Comparator(dataset,word):\n",
    "    global ReverseLookup\n",
    "    \n",
    "    PossibleCandidates = dict()\n",
    "\n",
    "    for key, value in dataset[word].items():\n",
    "        for chart_key,chart_value in ReverseLookup.items():\n",
    "            if key == chart_key:\n",
    "                for key, value in ReverseLookup[chart_key].items():\n",
    "\n",
    "                    if key in PossibleCandidates:\n",
    "                        pass\n",
    "                    else:\n",
    "                        PossibleCandidates[key] = value\n",
    "\n",
    "    ## Pass the suggested words dictionary for calculating suggestions\n",
    "    Final_suggestion(PossibleCandidates,word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final_suggestion function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives the final suggestions based on distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Gives the final suggestions based on distance calculation\n",
    "def Final_suggestion(PossibleCandidates,word):\n",
    "\n",
    "    global num_of_suggestions\n",
    "\n",
    "    distance_dict = dict()      # Dictionary to store the words and their edit distances\n",
    "    \n",
    "    for key, value in PossibleCandidates.items():   # Loop through all the possible candidates\n",
    "        distance = edit_distance(key,word)          # Find the edit distance from each one of them\n",
    "\n",
    "        distance_dict[key] = distance               # Put the suggested word with its edit distance\n",
    "\n",
    "    ## Sort the dictionary based on edit distance\n",
    "    sorted_dict = sorted(distance_dict.items(), key=operator.itemgetter(1))     \n",
    "\n",
    "    ## Display the top k suggestions\n",
    "    print(\"\\nTop \"+str(num_of_suggestions)+\" suggestions for \\\"\"+word+\"\\\" are : \")\n",
    "    print([(k[0]) for k in sorted_dict[:num_of_suggestions]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def substitution_error(b1,b2):\n",
    "    if b1 == b2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "## This function calculates the Levenshtein Distance\n",
    "def edit_distance(v, w):\n",
    "    matrix = [[0 for j in range(len(w) + 1)] for i in range(len(v) + 1)]\n",
    "    for i in range(len(v)+1):\n",
    "        for j in range(len(w)+1):\n",
    "            if i > 0 and j > 0:\n",
    "                val1 = matrix[i-1][j] + 1\n",
    "                val2 = matrix[i][j-1] + 1\n",
    "                val3 = matrix[i-1][j-1] + substitution_error(v[i-1],w[j-1]) \n",
    "                matrix[i][j] = min(val1, val2, val3)\n",
    "            elif i > 0:\n",
    "                matrix[i][j] = matrix[i-1][j] + 1\n",
    "            elif j > 0:\n",
    "                matrix[i][j] = matrix[i][j-1] + 1\n",
    "            else:\n",
    "                matrix[i][j] = 0 \n",
    "\n",
    "\n",
    "    return matrix[len(v)][len(w)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitting function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the sentence, checks each individual word if it is already correct or not. If it not a valid word then sends it for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Splits the sentence, checks each individual word if it is already correct or not\n",
    "## If it not a valid word then sends it for processing\n",
    "def Sentence_splitting(sentence):\n",
    "\n",
    "    words_set = set(words.words())  # Converts it into a set for faster lookup time\n",
    "    \n",
    "    text_tokens = sentence.split()  # Split the sentence\n",
    "\n",
    "    ## Loop through all the tokens\n",
    "    for i in range(len(text_tokens)):\n",
    "        if text_tokens[i] in words_set:     # If it is a valid word\n",
    "            print(\"\\n\\\"\"+text_tokens[i]+\"\\\"\"+\" is a valid word\")\n",
    "\n",
    "        ## If not a valid word\n",
    "        else:\n",
    "            tokenized_values = Tokenizer(text_tokens[i])\n",
    "            Ngrams = NgramCalculator(tokenized_values)\n",
    "            Comparator(Ngrams,text_tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Takes input from the user the text string\n",
    "def Take_input():\n",
    "\n",
    "    ## First ask the user how many word suggestions he wants\n",
    "    global num_of_suggestions\n",
    "    num_of_suggestions = int(input(\"\\nEnter the number of suggested words you want to be displayed : \"))\n",
    "\n",
    "    ## Loop until the user exits by pressing 'q'\n",
    "    while True:\n",
    "        print(\"\\n\\nEnter the string to check its words (Press 'q' to quit)\")\n",
    "        string = input(\"--> \")\n",
    "\n",
    "        ## If 'q' pressed then program quits\n",
    "        if string.strip().lower() == 'q':\n",
    "            print(\"Goodbye :)\")\n",
    "            exit()\n",
    "\n",
    "        ## Clean the text and send it for processing\n",
    "        else:\n",
    "            cleaned_text = string.strip().lower()\n",
    "            Sentence_splitting(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on UDHR corpus, Please Wait . . .\n",
      "\n",
      "Training Done.\n",
      "\n",
      "Training on Gulliver's Travels book, Please Wait . . .\n",
      "\n",
      "Training Done.\n",
      "\n",
      "Enter the number of suggested words you want to be displayed : 3\n",
      "\n",
      "\n",
      "Enter the string to check its words (Press 'q' to quit)\n",
      "--> ths\n",
      "\n",
      "Top 3 suggestions for \"ths\" are : \n",
      "['this', 'thus', 'paths']\n",
      "\n",
      "\n",
      "Enter the string to check its words (Press 'q' to quit)\n",
      "--> the blak spot on th forhed\n",
      "\n",
      "\"the\" is a valid word\n",
      "\n",
      "Top 3 suggestions for \"blak\" are : \n",
      "['black', 'beak', 'blast']\n",
      "\n",
      "\"spot\" is a valid word\n",
      "\n",
      "\"on\" is a valid word\n",
      "\n",
      "\"th\" is a valid word\n",
      "\n",
      "Top 3 suggestions for \"forhed\" are : \n",
      "['formed', 'forced', 'former']\n",
      "\n",
      "\n",
      "Enter the string to check its words (Press 'q' to quit)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    ## Used UDHR corpus for training\n",
    "    print(\"\\nTraining on UDHR corpus, Please Wait . . .\")\n",
    "\n",
    "    var = udhr.raw(\"English-Latin1\")\n",
    "    tokenized_values = Tokenizer(var.lower())\n",
    "    Ngrams = NgramCalculator(tokenized_values)\n",
    "    aggregator(Ngrams)\n",
    "\n",
    "    print(\"\\nTraining Done.\")\n",
    "\n",
    "    ## Used Gulliver's Travels for training as well to increase my training data set\n",
    "    print(\"\\nTraining on Gulliver's Travels book, Please Wait . . .\")\n",
    "\n",
    "    file = open(\"Gulliver.txt\",\"r\",encoding=\"utf-8\")\n",
    "    for line in file:\n",
    "        tokenized_values = Tokenizer(line.strip().lower())\n",
    "        Ngrams = NgramCalculator(tokenized_values)\n",
    "        aggregator(Ngrams)\n",
    "\n",
    "    print(\"\\nTraining Done.\")\n",
    "\n",
    "    ## Starts taking input from the user\n",
    "    Take_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
